<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NNMF Mathematical Details</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    <style>
        body {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        
        .main-container {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            margin: 20px;
            padding: 30px;
        }
        
        .header {
            text-align: center;
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 3px solid #667eea;
        }
        
        .header h1 {
            color: #333;
            font-weight: 700;
            margin-bottom: 10px;
        }
        
        .math-section {
            background: #f8f9fa;
            border-radius: 15px;
            padding: 25px;
            margin: 30px 0;
            border-left: 5px solid #667eea;
        }
        
        .step-box {
            background: white;
            border-radius: 10px;
            padding: 20px;
            margin: 15px 0;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            border-left: 4px solid #667eea;
        }
        
        .matrix-display {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
            font-family: 'Courier New', monospace;
            text-align: center;
            border: 2px solid #e9ecef;
        }
        
        .formula-large {
            font-size: 2em;
            text-align: center;
            color: #667eea;
            font-weight: bold;
            margin: 20px 0;
            padding: 20px;
            background: white;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        
        .btn-custom {
            background: linear-gradient(135deg, #667eea, #764ba2);
            border: none;
            color: white;
            padding: 12px 30px;
            border-radius: 25px;
            font-weight: 600;
            transition: all 0.3s ease;
        }
        
        .btn-custom:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
            color: white;
        }
        
        .calculation-step {
            background: white;
            border-radius: 10px;
            padding: 20px;
            margin: 15px 0;
            border: 2px solid #e9ecef;
        }
        
        .calculation-step h5 {
            color: #667eea;
            font-weight: bold;
            margin-bottom: 15px;
        }
        
        .result-highlight {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 15px;
            border-radius: 10px;
            margin: 10px 0;
            text-align: center;
            font-weight: bold;
        }
        
        .nav-link {
            color: #667eea;
            font-weight: 600;
        }
        
        .nav-link:hover {
            color: #764ba2;
        }
    </style>
</head>
<body>
    <div class="container-fluid">
        <div class="main-container">
            <div class="header">
                <h1><i class="fas fa-calculator"></i> NNMF Mathematical Details</h1>
                <p>Complete mathematical derivation with 2√ó2 example</p>
                <nav class="nav justify-content-center">
                    <a class="nav-link" href="/"><i class="fas fa-home"></i> Home</a>
                    <a class="nav-link" href="/svd"><i class="fas fa-cube"></i> SVD Visualization</a>
                    <a class="nav-link" href="/svd-math"><i class="fas fa-calculator"></i> SVD Mathematics</a>
                    <a class="nav-link" href="/nnmf"><i class="fas fa-layer-group"></i> NNMF Visualization</a>
                    <a class="nav-link" href="/pca"><i class="fas fa-compress"></i> PCA</a>
                    <a class="nav-link" href="/tsne"><i class="fas fa-project-diagram"></i> t-SNE</a>
                    <a class="nav-link" href="#theory">Theory</a>
                    <a class="nav-link" href="#example">2√ó2 Example</a>
                    <a class="nav-link" href="#calculations">Step-by-Step</a>
                </nav>
            </div>
            
            <div class="math-section">
                <h2><i class="fas fa-graduation-cap"></i> What is NNMF? (Beginner's Guide)</h2>
                
                <div class="step-box">
                    <button class="btn btn-outline-primary w-100 mb-3" type="button" data-bs-toggle="collapse" data-bs-target="#beginnerExplanation" aria-expanded="false" aria-controls="beginnerExplanation">
                        <i class="fas fa-info-circle"></i> Click here to learn what NNMF is (for beginners)
                    </button>
                    
                    <div class="collapse" id="beginnerExplanation">
                        <div class="card card-body">
                            <h5>üéØ What is Non-Negative Matrix Factorization (NNMF)?</h5>
                            
                            <h6>üìö Simple Analogy:</h6>
                            <p>Imagine you have a recipe book with 100 recipes, and you want to find the basic ingredients that make up all these recipes. NNMF is like finding the "fundamental ingredients" and "how much of each ingredient" goes into each recipe.</p>
                            
                            <h6>üî¢ In Mathematical Terms:</h6>
                            <p>You have a big table (matrix) of data, and you want to break it down into two smaller tables that, when multiplied together, recreate the original data as closely as possible.</p>
                            
                            <h6>üé® Real-World Example:</h6>
                            <p><strong>Image Analysis:</strong> If you have 1000 photos, NNMF can find the basic "patterns" (like edges, textures) and tell you how much of each pattern appears in each photo.</p>
                            
                            <h6>üéµ Music Example:</h6>
                            <p><strong>Music Decomposition:</strong> If you have 100 songs, NNMF can find the basic "musical elements" (like drum patterns, chord progressions) and show you how much of each element is in each song.</p>
                            
                            <h6>üìä Why "Non-Negative"?</h6>
                            <p>Unlike regular math where you can have negative numbers, NNMF only uses positive numbers (‚â• 0). This makes the results more interpretable:</p>
                            <ul>
                                <li>‚ùå Negative values: "This photo has -30% of edge patterns" (doesn't make sense!)</li>
                                <li>‚úÖ Non-negative values: "This photo has 30% of edge patterns" (makes perfect sense!)</li>
                            </ul>
                            
                            <h6>üéØ What Does NNMF Do?</h6>
                            <ol>
                                <li><strong>Finds Patterns:</strong> Discovers the fundamental building blocks in your data</li>
                                <li><strong>Shows Relationships:</strong> Tells you how much each pattern contributes to each data point</li>
                                <li><strong>Compresses Data:</strong> Reduces a large table into two smaller, more manageable tables</li>
                                <li><strong>Enables Understanding:</strong> Makes complex data interpretable and meaningful</li>
                            </ol>
                            
                            <h6>üîç Step-by-Step Process:</h6>
                            <ol>
                                <li><strong>Start:</strong> You have a big data table (like 1000 photos)</li>
                                <li><strong>Guess:</strong> Randomly create two smaller tables (patterns + amounts)</li>
                                <li><strong>Multiply:</strong> Multiply the two tables to recreate your original data</li>
                                <li><strong>Compare:</strong> See how close your recreation is to the original</li>
                                <li><strong>Improve:</strong> Adjust the tables to make the recreation better</li>
                                <li><strong>Repeat:</strong> Keep improving until you can't get much better</li>
                            </ol>
                            
                            <h6>üí° Why Is This Useful?</h6>
                            <ul>
                                <li><strong>Data Compression:</strong> Store 1000 photos as 10 patterns + 1000 small lists</li>
                                <li><strong>Pattern Discovery:</strong> Find hidden structures in your data</li>
                                <li><strong>Noise Reduction:</strong> Remove unimportant details, keep the important stuff</li>
                                <li><strong>Interpretability:</strong> Understand what makes your data special</li>
                            </ul>
                            
                            <h6>üéì TLDR:</h6>
                            <p>Think of NNMF as a way to find the "principal components" of your data, but with the constraint that everything must be positive. It's like PCA (Principal Component Analysis) but more interpretable because you can't have negative contributions.</p>
                            
                            <div class="alert alert-info mt-3">
                                <strong>üí° Key Takeaway:</strong> NNMF takes complex data and breaks it down into simple, interpretable parts that you can understand and work with!
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div id="theory" class="math-section">
                <h2><i class="fas fa-book"></i> Mathematical Theory</h2>
                
                <h4>What is NNMF?</h4>
                <p>Non-Negative Matrix Factorization (NNMF) decomposes a non-negative matrix A into two non-negative matrices:</p>
                
                <div class="formula-large">
                    A ‚âà W H
                </div>
                
                <h5>Where:</h5>
                <ul>
                    <li><strong>A</strong>: Original matrix (m√ón) with A ‚â• 0</li>
                    <li><strong>W</strong>: Coefficient matrix (m√ók) with W ‚â• 0</li>
                    <li><strong>H</strong>: Basis matrix (k√ón) with H ‚â• 0</li>
                </ul>
                
                <h4>Key Properties:</h4>
                <ul>
                    <li>All matrices have non-negative entries: A ‚â• 0, W ‚â• 0, H ‚â• 0</li>
                    <li>k is the rank of the factorization (number of components)</li>
                    <li>W represents how much each basis is used for each sample</li>
                    <li>H represents the fundamental patterns/basis vectors</li>
                </ul>
                
                <h4>Optimization Problem:</h4>
                <div class="step-box">
                    <h5>Objective Function</h5>
                    <p><strong>Minimize:</strong> $||A - WH||_F^2 = \sum_{i,j} (A_{ij} - (WH)_{ij})^2$</p>
                    <p><strong>Subject to:</strong> $W \geq 0, H \geq 0$</p>
                    
                    <h6>Detailed Explanation of Each Term:</h6>
                    <ul>
                        <li><strong>$A$</strong> - Original data matrix (m√ón)
                            <ul>
                                <li>Contains the data we want to factorize</li>
                                <li>Each element $A_{ij}$ represents a data point</li>
                            </ul>
                        </li>
                        <li><strong>$W$</strong> - Basis matrix (m√ók)
                            <ul>
                                <li>Contains the "basis vectors" or "features"</li>
                                <li>Each column represents a fundamental pattern</li>
                                <li>$W_{ik}$ = how much feature $k$ contributes to sample $i$</li>
                            </ul>
                        </li>
                        <li><strong>$H$</strong> - Coefficient matrix (k√ón)
                            <ul>
                                <li>Contains the "coefficients" or "encodings"</li>
                                <li>Each row represents how much each basis is used</li>
                                <li>$H_{kj}$ = how much basis $k$ contributes to feature $j$</li>
                            </ul>
                        </li>
                        <li><strong>$WH$</strong> - Reconstruction matrix (m√ón)
                            <ul>
                                <li>Matrix product of $W$ and $H$</li>
                                <li>Our approximation of the original matrix $A$</li>
                                <li>$(WH)_{ij} = \sum_{k=1}^{r} W_{ik} H_{kj}$</li>
                            </ul>
                        </li>
                        <li><strong>$|| \cdot ||_F$</strong> - Frobenius norm
                            <ul>
                                <li>Measures the "size" of a matrix</li>
                                <li>$||X||_F = \sqrt{\sum_{i,j} |X_{ij}|^2}$</li>
                                <li>Like Euclidean distance but for matrices</li>
                            </ul>
                        </li>
                        <li><strong>$||A - WH||_F^2$</strong> - Squared reconstruction error
                            <ul>
                                <li>Measures how far our approximation $WH$ is from original $A$</li>
                                <li>We square it to make optimization easier (removes square root)</li>
                                <li>Smaller values = better approximation</li>
                            </ul>
                        </li>
                        <li><strong>$\sum_{i,j}$</strong> - Sum over all matrix elements
                            <ul>
                                <li>Sum over all rows $i$ and columns $j$</li>
                                <li>Adds up the error for every single element</li>
                            </ul>
                        </li>
                        <li><strong>$(A_{ij} - (WH)_{ij})^2$</strong> - Element-wise squared error
                            <ul>
                                <li>Difference between original and reconstructed value</li>
                                <li>Squared to penalize large errors more heavily</li>
                                <li>Always non-negative (since it's squared)</li>
                            </ul>
                        </li>
                        <li><strong>$W \geq 0, H \geq 0$</strong> - Non-negativity constraints
                            <ul>
                                <li>All elements of $W$ and $H$ must be ‚â• 0</li>
                                <li>This is what makes it "Non-Negative" Matrix Factorization</li>
                                <li>Ensures interpretable, additive parts</li>
                            </ul>
                        </li>
                    </ul>
                </div>
                
                <div class="step-box">
                    <h5>Multiplicative Update Rules</h5>
                    <p><strong>Update W:</strong> $$W_{ik} \leftarrow W_{ik} \frac{(AH^T)_{ik}}{(WHH^T)_{ik}}$$</p>
                    <p><strong>Update H:</strong> $$H_{kj} \leftarrow H_{kj} \frac{(W^TA)_{kj}}{(W^TWH)_{kj}}$$</p>
                    
                    <h6>Explanation of Update Rules:</h6>
                    <ul>
                        <li><strong>Why multiplicative?</strong>
                            <ul>
                                <li>If current value is 0, it stays 0 (preserves non-negativity)</li>
                                <li>If current value is positive, it gets scaled up or down</li>
                                <li>Natural way to maintain non-negative constraints</li>
                            </ul>
                        </li>
                        <li><strong>Numerator $(AH^T)_{ik}$:</strong>
                            <ul>
                                <li>Measures how well current $H$ explains the data</li>
                                <li>Large values = good fit, should increase $W_{ik}$</li>
                                <li>$(AH^T)_{ik} = \sum_j A_{ij} H_{kj}$</li>
                            </ul>
                        </li>
                        <li><strong>Denominator $(WHH^T)_{ik}$:</strong>
                            <ul>
                                <li>Normalization term to prevent values from growing too large</li>
                                <li>$(WHH^T)_{ik} = \sum_j (WH)_{ij} H_{kj}$</li>
                                <li>Keeps the optimization stable</li>
                            </ul>
                        </li>
                        <li><strong>Ratio $\frac{\text{numerator}}{\text{denominator}}$:</strong>
                            <ul>
                                <li>If numerator > denominator ‚Üí increase $W_{ik}$</li>
                                <li>If numerator < denominator ‚Üí decrease $W_{ik}$</li>
                                <li>If numerator = denominator ‚Üí keep $W_{ik}$ unchanged</li>
                            </ul>
                        </li>
                        <li><strong>Why alternating updates?</strong>
                            <ul>
                                <li>Update $H$ while keeping $W$ fixed</li>
                                <li>Then update $W$ while keeping $H$ fixed</li>
                                <li>Easier than optimizing both simultaneously</li>
                            </ul>
                        </li>
                    </ul>
                </div>
                
                <div class="step-box">
                    <h5>Algorithm Steps</h5>
                    <p>1. Initialize W and H with random non-negative values</p>
                    <p>2. Repeat until convergence:</p>
                    <p>&nbsp;&nbsp;&nbsp;&nbsp;a. Update H using multiplicative rule</p>
                    <p>&nbsp;&nbsp;&nbsp;&nbsp;b. Update W using multiplicative rule</p>
                    <p>&nbsp;&nbsp;&nbsp;&nbsp;c. Check convergence criterion</p>
                </div>
            </div>
            
            <div id="example" class="math-section">
                <h2><i class="fas fa-cube"></i> 2√ó2 Example</h2>
                <p>Let's work through a complete example with matrix:</p>
                
                <div class="matrix-display" id="matrix-A">
                    <h5>Matrix A:</h5>
                    <div id="matrix-A-content">Loading...</div>
                </div>
                
                <button class="btn btn-custom" onclick="loadExample()">
                    <i class="fas fa-play"></i> Compute NNMF Step by Step
                </button>
                
                <div id="calculations" style="display: none;">
                    <h3><i class="fas fa-list-ol"></i> Complete Step-by-Step Calculations</h3>
                    
                    <div class="calculation-step">
                        <h5>Step 1: Initialize Matrices</h5>
                        <div id="step1-content"></div>
                    </div>
                    
                    <div class="calculation-step">
                        <h5>Step 2: First Iteration - Update H</h5>
                        <div id="step2-content"></div>
                    </div>
                    
                    <div class="calculation-step">
                        <h5>Step 3: First Iteration - Update W</h5>
                        <div id="step3-content"></div>
                    </div>
                    
                    <div class="calculation-step">
                        <h5>Step 4: Compute Reconstruction Error</h5>
                        <div id="step4-content"></div>
                    </div>
                    
                    <div class="calculation-step">
                        <h5>Step 5: Continue Iterations</h5>
                        <div id="step5-content"></div>
                    </div>
                    
                    <div class="calculation-step">
                        <h5>Step 6: Final Results</h5>
                        <div id="step6-content"></div>
                    </div>
                    
                    <div class="result-highlight">
                        <h4>Final NNMF Decomposition:</h4>
                        <div id="final-result"></div>
                    </div>
                </div>
            </div>
            
            <div class="math-section">
                <h2><i class="fas fa-lightbulb"></i> Key Insights</h2>
                
                <h4>Why Non-Negativity?</h4>
                <p>The non-negativity constraint makes NNMF particularly useful for:</p>
                <ul>
                    <li><strong>Interpretability</strong>: Components represent additive parts</li>
                    <li><strong>Sparsity</strong>: Often leads to sparse representations</li>
                    <li><strong>Count Data</strong>: Natural for data that can't be negative</li>
                    <li><strong>Images</strong>: Pixel intensities are non-negative</li>
                </ul>
                
                <h4>Multiplicative Updates Explained:</h4>
                <p>The multiplicative update rules ensure:</p>
                <ul>
                    <li><strong>Non-negativity preservation</strong>: If W,H ‚â• 0 initially, they stay ‚â• 0</li>
                    <li><strong>Monotonic convergence</strong>: Error decreases with each iteration</li>
                    <li><strong>No normalization needed</strong>: Updates are self-normalizing</li>
                </ul>
                
                <h4>Convergence Criteria:</h4>
                <ul>
                    <li><strong>Relative change</strong>: $$\frac{|\text{error}_{new} - \text{error}_{old}|}{\text{error}_{old}} < \epsilon$$</li>
                    <li><strong>Maximum iterations</strong>: Stop after max_iter steps</li>
                    <li><strong>Gradient magnitude</strong>: When gradient becomes small</li>
                </ul>
                
                <h4>Advantages over SVD:</h4>
                <ul>
                    <li><strong>Interpretability</strong>: Components are additive, not subtractive</li>
                    <li><strong>Sparsity</strong>: Often produces sparser solutions</li>
                    <li><strong>Non-negativity</strong>: Natural for many data types</li>
                    <li><strong>Parts-based representation</strong>: Components represent "parts" of data</li>
                </ul>
                
                <h4>Applications:</h4>
                <ul>
                    <li><strong>Image Processing</strong>: Face recognition, image compression</li>
                    <li><strong>Text Mining</strong>: Topic modeling, document clustering</li>
                    <li><strong>Bioinformatics</strong>: Gene expression analysis</li>
                    <li><strong>Recommendation Systems</strong>: Collaborative filtering</li>
                    <li><strong>Audio Processing</strong>: Source separation</li>
                </ul>
            </div>
        </div>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <script>
        async function loadExample() {
            try {
                const response = await fetch('/api/nnmf-example');
                const data = await response.json();
                
                // Display matrix A
                document.getElementById('matrix-A-content').innerHTML = 
                    `$$A = \\begin{bmatrix} ${data.matrix_A[0][0]} & ${data.matrix_A[0][1]} \\\\ ${data.matrix_A[1][0]} & ${data.matrix_A[1][1]} \\end{bmatrix}$$`;
                
                const steps = data.detailed_steps;
                
                // Debug: Check for dollar signs in the strings
                console.log('Debug - Step 2 calculate_WT:', JSON.stringify(steps.step2.calculate_WT));
                console.log('Debug - Step 2 calculate_WTA:', JSON.stringify(steps.step2.calculate_WTA));
                
                // Step 1: Initialize Matrices
                document.getElementById('step1-content').innerHTML = 
                    `$$${steps.step1.initialization}$$<br><br>
                     <strong>Note:</strong> ${steps.step1.note}<br>
                     <strong>Check:</strong> ${steps.step1.check_non_negative}`;
                
                // Step 2: Update H
                document.getElementById('step2-content').innerHTML = 
                    `<strong>Step 2a: Calculate W^T</strong><br>
                     \\[${steps.step2.calculate_WT}\\]<br><br>
                     <strong>Step 2b: Calculate W^T A</strong><br>
                     \\[${steps.step2.calculate_WTA}\\]<br><br>
                     <strong>Step 2c: Calculate W^T W</strong><br>
                     \\[${steps.step2.calculate_WTW}\\]<br><br>
                     <strong>Step 2d: Calculate W^T W H</strong><br>
                     \\[${steps.step2.calculate_WTWH}\\]<br><br>
                     <strong>Step 2e: Update H using multiplicative rule</strong><br>
                     \\[${steps.step2.update_H}\\]`;
                
                // Step 3: Update W
                document.getElementById('step3-content').innerHTML = 
                    `<strong>Step 3a: Calculate H^T</strong><br>
                     $${steps.step3.calculate_HT}$$<br><br>
                     <strong>Step 3b: Calculate A H^T</strong><br>
                     $${steps.step3.calculate_AHT}$$<br><br>
                     <strong>Step 3c: Calculate W H H^T</strong><br>
                     $${steps.step3.calculate_WHHT}$$<br><br>
                     <strong>Step 3d: Update W using multiplicative rule</strong><br>
                     $${steps.step3.update_W}$$`;
                
                // Step 4: Compute Error
                document.getElementById('step4-content').innerHTML = 
                    `<strong>Step 4a: Calculate W^(1) H^(1)</strong><br>
                     $${steps.step4.calculate_WH}$$<br><br>
                     <strong>Step 4b: Calculate Reconstruction Error</strong><br>
                     $${steps.step4.error_calculation}$$`;
                
                // Step 5: Continue Iterations
                document.getElementById('step5-content').innerHTML = 
                    `<strong>Continue the process:</strong><br>
                     ‚Ä¢ Repeat steps 2-4 for more iterations<br>
                     ‚Ä¢ Each iteration improves the approximation<br>
                     ‚Ä¢ Stop when error change is small enough<br>
                     ‚Ä¢ Typical convergence: 50-200 iterations`;
                
                // Step 6: Final Results
                document.getElementById('step6-content').innerHTML = 
                    `<strong>Final converged matrices:</strong><br>
                     $$W^{\\text{final}} = \\begin{bmatrix} ${data.updated_W[0][0].toFixed(3)} & ${data.updated_W[0][1].toFixed(3)} \\\\ ${data.updated_W[1][0].toFixed(3)} & ${data.updated_W[1][1].toFixed(3)} \\end{bmatrix}$$<br>
                     $$H^{\\text{final}} = \\begin{bmatrix} ${data.updated_H[0][0].toFixed(3)} & ${data.updated_H[0][1].toFixed(3)} \\\\ ${data.updated_H[1][0].toFixed(3)} & ${data.updated_H[1][1].toFixed(3)} \\end{bmatrix}$$<br>
                     <strong>Final reconstruction error:</strong> ${data.error.toFixed(3)}`;
                
                // Final result
                document.getElementById('final-result').innerHTML = 
                    `$$A \\approx W^{\\text{final}} H^{\\text{final}} = \\begin{bmatrix} ${data.updated_W[0][0].toFixed(3)} & ${data.updated_W[0][1].toFixed(3)} \\\\ ${data.updated_W[1][0].toFixed(3)} & ${data.updated_W[1][1].toFixed(3)} \\end{bmatrix} \\begin{bmatrix} ${data.updated_H[0][0].toFixed(3)} & ${data.updated_H[0][1].toFixed(3)} \\\\ ${data.updated_H[1][0].toFixed(3)} & ${data.updated_H[1][1].toFixed(3)} \\end{bmatrix} = \\begin{bmatrix} ${data.reconstruction[0][0].toFixed(3)} & ${data.reconstruction[0][1].toFixed(3)} \\\\ ${data.reconstruction[1][0].toFixed(3)} & ${data.reconstruction[1][1].toFixed(3)} \\end{bmatrix}$$`;
                
                // Show calculations
                document.getElementById('calculations').style.display = 'block';
                
                // Re-render MathJax
                MathJax.typesetPromise();
                
            } catch (error) {
                console.error('Error loading example:', error);
                alert('Error loading mathematical example: ' + error.message);
            }
        }
        
        // Load initial matrix display
        window.onload = function() {
            document.getElementById('matrix-A-content').innerHTML = 
                `$$A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}$$`;
            MathJax.typesetPromise();
        };
    </script>
</body>
</html>
